{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "from pylab import *\n",
    "import random\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import Conv2D, ZeroPadding2D, Activation, Input, concatenate\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D\n",
    "from keras.layers.merge import Concatenate\n",
    "from keras.layers.core import Lambda, Flatten, Dense\n",
    "from keras.initializers import glorot_uniform,he_uniform\n",
    "\n",
    "from keras.engine.topology import Layer\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "from keras.utils import plot_model,normalize\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from triplet_loss_function import triplet_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking shapes for class 0 (train) :  (5923, 28, 28, 1)\n",
      "Checking shapes for class 0 (test) :  (980, 28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "nb_classes = 10\n",
    "img_rows, img_cols = 28, 28\n",
    "input_shape = (img_rows, img_cols, 1)\n",
    "\n",
    "def buildDataSet():\n",
    "    \"\"\"Build dataset for train and test\n",
    "    \n",
    "    \n",
    "    returns:\n",
    "        dataset : list of lengh 10 containing images for each classes of shape (?,28,28,1)\n",
    "    \"\"\"\n",
    "    (x_train_origin, y_train_origin), (x_test_origin, y_test_origin) = mnist.load_data()\n",
    "\n",
    "    assert K.image_data_format() == 'channels_last'\n",
    "    x_train_origin = x_train_origin.reshape(x_train_origin.shape[0], img_rows, img_cols, 1)\n",
    "    x_test_origin = x_test_origin.reshape(x_test_origin.shape[0], img_rows, img_cols, 1)\n",
    "    \n",
    "    dataset_train = []\n",
    "    dataset_test = []\n",
    "    \n",
    "    #Sorting images by classes and normalize values 0=>1\n",
    "    for n in range(nb_classes):\n",
    "        images_class_n = np.asarray([row for idx,row in enumerate(x_train_origin) if y_train_origin[idx]==n])\n",
    "        dataset_train.append(images_class_n/255)\n",
    "        \n",
    "        images_class_n = np.asarray([row for idx,row in enumerate(x_test_origin) if y_test_origin[idx]==n])\n",
    "        dataset_test.append(images_class_n/255)\n",
    "        \n",
    "    return dataset_train,dataset_test,x_train_origin,y_train_origin,x_test_origin,y_test_origin\n",
    "\n",
    "dataset_train,dataset_test,x_train_origin,y_train_origin,x_test_origin,y_test_origin = buildDataSet()\n",
    "print(\"Checking shapes for class 0 (train) : \",dataset_train[0].shape)\n",
    "print(\"Checking shapes for class 0 (test) : \",dataset_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_dist(a,b):\n",
    "    return np.sum(np.square(a-b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Triplets\n",
    "def get_batch_random(batch_size,s=\"train\"):\n",
    "    \"\"\"\n",
    "    Create batch of APN triplets with a complete random strategy\n",
    "    \n",
    "    Arguments:\n",
    "    batch_size -- integer \n",
    "\n",
    "    Returns:\n",
    "    triplets -- list containing 3 tensors A,P,N of shape (batch_size,w,h,c)\n",
    "    \"\"\"\n",
    "    if s == 'train':\n",
    "        X = dataset_train\n",
    "    else:\n",
    "        X = dataset_test\n",
    "\n",
    "    m, w, h,c = X[0].shape\n",
    "    \n",
    "    \n",
    "    # initialize result\n",
    "    triplets=[np.zeros((batch_size,h, w,c)) for i in range(3)]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        #Pick one random class for anchor\n",
    "        anchor_class = np.random.randint(0, nb_classes)\n",
    "        nb_sample_available_for_class_AP = X[anchor_class].shape[0]\n",
    "        \n",
    "        #Pick two different random pics for this class => A and P\n",
    "        [idx_A,idx_P] = np.random.choice(nb_sample_available_for_class_AP,size=2,replace=False)\n",
    "        \n",
    "        #Pick another class for N, different from anchor_class\n",
    "        negative_class = (anchor_class + np.random.randint(1,nb_classes)) % nb_classes\n",
    "        nb_sample_available_for_class_N = X[negative_class].shape[0]\n",
    "        \n",
    "        #Pick a random pic for this negative class => N\n",
    "        idx_N = np.random.randint(0, nb_sample_available_for_class_N)\n",
    "\n",
    "        triplets[0][i,:,:,:] = X[anchor_class][idx_A,:,:,:]\n",
    "        triplets[1][i,:,:,:] = X[anchor_class][idx_P,:,:,:]\n",
    "        triplets[2][i,:,:,:] = X[negative_class][idx_N,:,:,:]\n",
    "\n",
    "    return triplets\n",
    "\n",
    "def opt_triplet():\n",
    "    '''return triplet with similar neg. image'''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_network(input_shape, embeddingsize):\n",
    "    '''\n",
    "    Define the neural network to learn image similarity\n",
    "    Input : \n",
    "            input_shape : shape of input images\n",
    "            embeddingsize : vectorsize used to encode our picture   \n",
    "    '''\n",
    "     # Convolutional Neural Network\n",
    "    network = Sequential()\n",
    "    network.add(Conv2D(128, (7,7), activation='relu',\n",
    "                     input_shape=input_shape,\n",
    "                     kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "    network.add(Conv2D(128, (3,3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=l2(2e-4)))\n",
    "    network.add(MaxPooling2D())\n",
    "    network.add(Conv2D(256, (3,3), activation='relu', kernel_initializer='he_uniform',\n",
    "                     kernel_regularizer=l2(2e-4)))\n",
    "    network.add(Flatten())\n",
    "    network.add(Dense(4096, activation='relu',\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='he_uniform'))\n",
    "    \n",
    "    \n",
    "    network.add(Dense(embeddingsize, activation=None,\n",
    "                   kernel_regularizer=l2(1e-3),\n",
    "                   kernel_initializer='he_uniform'))\n",
    "    \n",
    "    #Force the encoding to live on the d-dimentional hypershpere\n",
    "    network.add(Lambda(lambda x: K.l2_normalize(x,axis=-1)))\n",
    "    \n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TripletLossLayer(Layer):\n",
    "    def __init__(self, alpha, **kwargs):\n",
    "        self.alpha = alpha\n",
    "        super(TripletLossLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def triplet_loss(self, inputs):\n",
    "        anchor, positive, negative = inputs\n",
    "        p_dist = K.sum(K.square(anchor-positive), axis=-1)\n",
    "        n_dist = K.sum(K.square(anchor-negative), axis=-1)\n",
    "        return K.sum(K.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        loss = self.triplet_loss(inputs)\n",
    "        self.add_loss(loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_shape, network, margin=0.2):\n",
    "    '''\n",
    "    Define the Keras Model for training \n",
    "        Input : \n",
    "            input_shape : shape of input images\n",
    "            network : Neural network to train outputing embeddings\n",
    "            margin : minimal distance between Anchor-Positive and Anchor-Negative for the lossfunction (alpha)\n",
    "    \n",
    "    '''\n",
    "     # Define the tensors for the three input images\n",
    "    anchor_input = Input(input_shape, name=\"anchor_input\")\n",
    "    positive_input = Input(input_shape, name=\"positive_input\")\n",
    "    negative_input = Input(input_shape, name=\"negative_input\") \n",
    "    \n",
    "    # Generate the encodings (feature vectors) for the three images\n",
    "    encoded_a = network(anchor_input)\n",
    "    encoded_p = network(positive_input)\n",
    "    encoded_n = network(negative_input)\n",
    "    \n",
    "    #TripletLoss Layer\n",
    "    loss_layer = TripletLossLayer(alpha=margin,name='triplet_loss_layer')([encoded_a,encoded_p,encoded_n])\n",
    "    \n",
    "    # Connect the inputs with the outputs\n",
    "    network_train = Model(inputs=[anchor_input,positive_input,negative_input],outputs=loss_layer)\n",
    "    \n",
    "    # return the model\n",
    "    return network_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/niklastecklenburg/opt/anaconda3/envs/mim/lib/python3.7/site-packages/keras/engine/training_utils.py:819: UserWarning: Output triplet_loss_layer missing from loss dictionary. We assume this was done on purpose. The fit and evaluate APIs will not be expecting any data to be passed to triplet_loss_layer.\n",
      "  'be expecting any data to be passed to {0}.'.format(name))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "anchor_input (InputLayer)       (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "positive_input (InputLayer)     (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "negative_input (InputLayer)     (None, 28, 28, 1)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential_2 (Sequential)       (None, 10)           4688522     anchor_input[0][0]               \n",
      "                                                                 positive_input[0][0]             \n",
      "                                                                 negative_input[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "triplet_loss_layer (TripletLoss [(None, 10), (None,  0           sequential_2[1][0]               \n",
      "                                                                 sequential_2[2][0]               \n",
      "                                                                 sequential_2[3][0]               \n",
      "==================================================================================================\n",
      "Total params: 4,688,522\n",
      "Trainable params: 4,688,522\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "['loss']\n"
     ]
    }
   ],
   "source": [
    "input_shape = (28,28,1)\n",
    "\n",
    "network = build_network(input_shape,embeddingsize=10)\n",
    "network_train = build_model(input_shape,network)\n",
    "optimizer = Adam(lr = 0.00006)\n",
    "network_train.compile(loss=None,optimizer=optimizer)\n",
    "network_train.summary()\n",
    "plot_model(network_train,show_shapes=True, show_layer_names=True, to_file='02 model.png')\n",
    "print(network_train.metrics_names)\n",
    "n_iteration=0\n",
    "network_train.load_weights('mnist-160k_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_probs(network,X,Y):\n",
    "    '''\n",
    "    Input\n",
    "        network : current NN to compute embeddings\n",
    "        X : tensor of shape (m,w,h,1) containing pics to evaluate\n",
    "        Y : tensor of shape (m,) containing true class\n",
    "        \n",
    "    Returns\n",
    "        probs : array of shape (m,m) containing distances\n",
    "    \n",
    "    '''\n",
    "    m = X.shape[0]\n",
    "    nbevaluation = int(m*(m-1)/2)\n",
    "    probs = np.zeros((nbevaluation))\n",
    "    y = np.zeros((nbevaluation))\n",
    "    \n",
    "    #Compute all embeddings for all pics with current network\n",
    "    embeddings = network.predict(X)\n",
    "    \n",
    "    size_embedding = embeddings.shape[1]\n",
    "    \n",
    "    #For each pics of our dataset\n",
    "    k = 0\n",
    "    for i in range(m):\n",
    "            #Against all other images\n",
    "            for j in range(i+1,m):\n",
    "                #compute the probability of being the right decision : it should be 1 for right class, 0 for all other classes\n",
    "                probs[k] = -compute_dist(embeddings[i,:],embeddings[j,:])\n",
    "                if (Y[i]==Y[j]):\n",
    "                    y[k] = 1\n",
    "                    #print(\"{3}:{0} vs {1} : {2}\\tSAME\".format(i,j,probs[k],k))\n",
    "                else:\n",
    "                    y[k] = 0\n",
    "                    #print(\"{3}:{0} vs {1} : \\t\\t\\t{2}\\tDIFF\".format(i,j,probs[k],k))\n",
    "                k += 1\n",
    "    return probs,y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training process!\n",
      "-------------------------------------\n",
      "\n",
      " ------------- \n",
      "\n",
      "[3] Time for 0 iterations: 0.0 mins, Train Loss: 0.0986684262752533\n",
      "\n",
      " ------------- \n",
      "\n",
      "[4] Time for 1 iterations: 0.0 mins, Train Loss: 0.0984417200088501\n",
      "\n",
      " ------------- \n",
      "\n",
      "[5] Time for 2 iterations: 0.0 mins, Train Loss: 0.09820482134819031\n",
      "\n",
      " ------------- \n",
      "\n",
      "[6] Time for 3 iterations: 0.0 mins, Train Loss: 0.09796139597892761\n",
      "\n",
      " ------------- \n",
      "\n",
      "[7] Time for 4 iterations: 0.0 mins, Train Loss: 0.09771363437175751\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting training process!\")\n",
    "print(\"-------------------------------------\")\n",
    "\n",
    "evaluate_every = 1\n",
    "n_val = 5\n",
    "batch_size = 20\n",
    "\n",
    "t_start = time.time()\n",
    "for i in range(5):\n",
    "    #triplets = get_batch_hard(200,16,16,network)\n",
    "    triplets = get_batch_random(batch_size)\n",
    "    loss = network_train.train_on_batch(triplets, None)\n",
    "    n_iteration += 1\n",
    "    if i % evaluate_every == 0:\n",
    "        print(\"\\n ------------- \\n\")\n",
    "        print(\"[{3}] Time for {0} iterations: {1:.1f} mins, Train Loss: {2}\".format(i, (time.time()-t_start)/60.0,loss,n_iteration))\n",
    "        probs,yprob = compute_probs(network,x_test_origin[:n_val,:,:,:],y_test_origin[:n_val])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "80a48ae8c71c47635098f949858e5a595d8f35cf974b53732b89d151ad08617d"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('mim': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
